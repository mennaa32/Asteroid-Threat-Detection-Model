
import requests
from datetime import datetime, timedelta


import pandas as pd
import matplotlib.pyplot as plt


import seaborn as sns

# Feature Selection & Engineering
from sklearn.preprocessing import MinMaxScaler

# model and trinning
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# URL Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ù€ API
from datetime import datetime, timedelta

# Ø­Ù„ Ù…Ø´ÙƒÙ„Ù‡ Ø§Ù† Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ø®Ø·Ø± Ù‚Ù„ÙŠÙ„Ù‡
from imblearn.over_sampling import SMOTE

# ==============================
#        Data Collection
# ==============================


# Ø¬Ù„Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„ÙŠÙˆÙ…
today = datetime.now().strftime("%Y-%m-%d")

# Ø¬Ù„Ø¨ ØªØ§Ø±ÙŠØ® Ø¢Ø®Ø± 5 Ø£ÙŠØ§Ù…
five_days_ago = (datetime.now() - timedelta(days=5)).strftime("%Y-%m-%d")

# Ø±Ø¨Ø· Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¨Ø§Ù„Ù€ API
url = f"https://api.nasa.gov/neo/rest/v1/feed?start_date={five_days_ago}&end_date={today}&detailed=false&api_key=On1l7e6HF8kTWgZvZ9tjZTfCe4ZOdJ3ZceBNvmgn"

# Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ ÙˆØ¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
response = requests.get(url)
data = response.json()

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù€ JSON
neos = []
for date, objects in data["near_earth_objects"].items():
    for obj in objects:
        neos.append({
            "id": obj["id"],
            "name": obj["name"],
            "absolute_magnitude_h": obj["absolute_magnitude_h"],
            "estimated_diameter_min": obj["estimated_diameter"]["kilometers"]["estimated_diameter_min"],
            "estimated_diameter_max": obj["estimated_diameter"]["kilometers"]["estimated_diameter_max"],
            "is_potentially_hazardous_asteroid": obj["is_potentially_hazardous_asteroid"],
            "relative_velocity": float(obj["close_approach_data"][0]["relative_velocity"]["kilometers_per_hour"]),
            "miss_distance": float(obj["close_approach_data"][0]["miss_distance"]["kilometers"]),
            "orbiting_body": obj["close_approach_data"][0]["orbiting_body"],
            "close_approach_date": obj["close_approach_data"][0]["close_approach_date"]
        })

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ DataFrame
df = pd.DataFrame(neos)

# ØªØ­ÙˆÙŠÙ„ 'close_approach_date' Ø¥Ù„Ù‰ datetime
df['close_approach_date'] = pd.to_datetime(df['close_approach_date'])

# ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ 'close_approach_date' (Ù…Ù† Ø§Ù„Ø£Ù‚Ø¯Ù… Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø­Ø¯Ø«)
df_sorted = df.sort_values(by='close_approach_date', ascending=True)

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print(df.info())

# Ø¹Ø±Ø¶ Ø£Ø®Ø± 5 ØµÙÙˆÙ
print(df_sorted.tail())

# Ø¹Ø±Ø¶ Ø§ÙˆÙ„ 5 ØµÙÙˆÙ
print(df_sorted.head())


# ==============================
#    Exploratory Data Analysis (EDA)
# ==============================

# 1ï¸âƒ£ ÙˆØµÙ Ø¥Ø­ØµØ§Ø¦ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print(df.describe())

# 2ï¸âƒ£ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙˆÙŠÙƒØ¨Ø§Øª Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø®Ø·ÙˆØ±Ø©
sns.countplot(data=df, x='is_potentially_hazardous_asteroid')
plt.title('Distribution of Hazardous Asteroids')
plt.show()


# 4ï¸âƒ£ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ù…Ø³Ø§ÙØ§Øª Ø¹Ù† Ø§Ù„Ø£Ø±Ø¶
plt.figure(figsize=(8, 5))
sns.scatterplot(data=df, x='miss_distance', y='relative_velocity', hue='is_potentially_hazardous_asteroid')
plt.title('Miss Distance vs Relative Velocity')
plt.show()

# 5ï¸âƒ£ Ø¹Ø±Ø¶ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© Ø¥Ù† ÙˆØ¬Ø¯Øª
print("\nMissing Values:\n", df.isnull().sum())

# 6ï¸âƒ£ Ø¹Ø±Ø¶ Ø§Ù„Ù€ Duplicates Ø¥Ù† ÙˆØ¬Ø¯Øª
print("\nDuplicated Rows:\n", df.duplicated().sum())

# ==============================
#       Data Cleaning
# ==============================

# 1ï¸âƒ£ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù€ Duplicates Ø¥Ù† ÙˆØ¬Ø¯Øª
df.drop_duplicates(inplace=True)

# 2ï¸âƒ£ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù€ Missing Values Ø¥Ù† ÙˆØ¬Ø¯Øª
df.dropna(inplace=True)

# 3ï¸âƒ£ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù€ Data Types ÙˆØªØ¹Ø¯ÙŠÙ„Ù‡Ø§ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
print("\nData Types Before:\n", df.dtypes)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù…ÙˆØ¯ 'close_approach_date' Ø¥Ù„Ù‰ Ù†ÙˆØ¹ ØªØ§Ø±ÙŠØ®
df['close_approach_date'] = pd.to_datetime(df['close_approach_date'])

print("\nData Types After:\n", df.dtypes)

# 4ï¸âƒ£ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙØ­Øµ Ù„Ù„ØªØ£ÙƒØ¯ Ø£Ù† ÙƒÙ„ Ø´ÙŠØ¡ ØªÙ…Ø§Ù…
print("\nData after cleaning:\n")
print(df.info())
print(df.head())

# ==============================
# Feature Selection & Engineering
# ==============================


# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù€ Features Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
features = df[['absolute_magnitude_h', 'estimated_diameter_min', 'estimated_diameter_max',
               'relative_velocity', 'miss_distance']]

# ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Normalization)
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø±Ø© ØªØ§Ù†ÙŠØ© Ø¥Ù„Ù‰ DataFrame
features_df = pd.DataFrame(features_scaled, columns=features.columns)

# Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
print("\nFeatures after Scaling:\n")
print(features_df.head())

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù€ Target Column
features_df['is_potentially_hazardous_asteroid'] = df['is_potentially_hazardous_asteroid']

# Ø¹Ø±Ø¶ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print("\nFinal Dataset Ready for Modeling:\n")
print(features_df.head())


# ==============================
#       Model Selection & Training
# ==============================


# 1ï¸âƒ£ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Training Ùˆ Testing
X = features_df.drop('is_potentially_hazardous_asteroid', axis=1)
y = features_df['is_potentially_hazardous_asteroid']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 2ï¸âƒ£ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Random Forest Classifier)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 3ï¸âƒ£ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬
y_pred = model.predict(X_test)

# 4ï¸âƒ£ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# 5ï¸âƒ£ Confusion Matrix
# plt.figure(figsize=(5, 4))
# sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',
#             xticklabels=['Not Hazardous', 'Hazardous'],
#             yticklabels=['Not Hazardous', 'Hazardous'])
# plt.title('Confusion Matrix')
# plt.show()

plt.figure(figsize=(5, 4))
cm = confusion_matrix(y_test, y_pred, labels=[False, True])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Hazardous', 'Hazardous'],
            yticklabels=['Not Hazardous', 'Hazardous'])
plt.title('Confusion Matrix')
plt.show()


# ==============================
#       Feature Importance
# ==============================
plt.figure(figsize=(8, 5))
feature_importances = pd.Series(model.feature_importances_, index=X.columns)
feature_importances.nlargest(5).plot(kind='barh', color='skyblue')
plt.title('Feature Importances')
plt.show()




# ==============================
#       Handling Imbalanced Data
# ==============================

# ØªØ·Ø¨ÙŠÙ‚ SMOTE Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
smote = SMOTE(random_state=42, k_neighbors=1)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print(f"Before SMOTE: {y_train.value_counts()}")
print(f"After SMOTE: {y_resampled.value_counts()}")

# 1ï¸âƒ£ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ø¯ÙŠØ¯ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ§Ø²Ù†
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# 2ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Random Forest Classifier)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 3ï¸âƒ£ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬
y_pred = model.predict(X_test)

# 4ï¸âƒ£ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# 5ï¸âƒ£ Ø±Ø³Ù… Ø§Ù„Ù€ Confusion Matrix
plt.figure(figsize=(5, 4))
cm = confusion_matrix(y_test, y_pred, labels=[False, True])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Hazardous', 'Hazardous'],
            yticklabels=['Not Hazardous', 'Hazardous'])
plt.title('Confusion Matrix')
plt.show()




# ==============================
#       Manual Prediction
# ==============================

print("\nğŸ” Ø£Ø¯Ø®Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙˆÙŠÙƒØ¨ ÙŠØ¯ÙˆÙŠÙ‹Ø§:")

# Ø£Ø®Ø° Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
absolute_magnitude_h = float(input("Absolute Magnitude : "))
estimated_diameter_min = float(input("Estimated Diameter Min (km): "))
estimated_diameter_max = float(input("Estimated Diameter Max (km): "))
relative_velocity = float(input("Relative Velocity (km\h): "))
miss_distance = float(input("Miss Distance (km): "))

# Ø¨Ù†Ø§Ø¡ DataFrame Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
manual_input = pd.DataFrame([[
    absolute_magnitude_h,
    estimated_diameter_min,
    estimated_diameter_max,
    relative_velocity,
    miss_distance
]], columns=X.columns)

# ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø²ÙŠ Ù…Ø§ Ø¹Ù…Ù„Ù†Ø§ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
manual_input_scaled = scaler.transform(manual_input)

# Ø§Ù„ØªÙ†Ø¨Ø¤
manual_pred = model.predict(manual_input_scaled)

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
print("\nâš ï¸ result:")
if manual_pred[0]:
    print("ğŸš¨danger")
else:
    print("âœ… safe")
